extraListeners = com.hortonworks.spark.atlas.SparkAtlasEventTracker
sql.queryExecutionListeners = com.hortonworks.spark.atlas.SparkAtlasEventTracker

datasets = "/tmp/datasets"
datasets = ${?COMET_DATASETS}

tmpdir = "/tmp/comet_tmp"
tmpdir = ${?COMET_TMPDIR}

chewer-prefix = "comet.chewer"
chewer-prefix = ${?COMET_CHEWER_PREFIX}

metadata = "/tmp/metadata"
metadata = ${?COMET_METADATA}

launcher = simple
launcher = ${?COMET_LAUNCHER}

archive = true
archive = ${?COMET_ARCHIVE}

#file-system = "gs://comet-app"
file-system = "file://"
file-system = ${?COMET_FS}

hive = false
hive = ${?COMET_HIVE}

grouped = true
grouped = ${?COMET_GROUPED}

analyze = false
analyze = ${?COMET_ANALYZE}

airflow {
  ingest = "comet_ingest"
  ingest = ${?AIRFLOW_INGEST}

  endpoint = "http://127.0.0.1:8080/api/experimental"
  endpoint = ${?AIRFLOW_ENDPOINT}
}

writeFormat = parquet
writeFormat = ${?COMET_WRITE_FORMAT}


merge-force-distinct = false
merge-force-distinct = ${?COMET_MERGE_FORCE_DISTINCT}


lock {
  path = "/tmp/locks"
  path = ${?COMET_LOCK_PATH}

  metrics-timeout = -1
  metrics-timeout = ${?COMET_LOCK_METRICS_TIMEOUT}

  ingestion-timeout = -1
  ingestion-timeout = ${?COMET_LOCK_INGESTION_TIMEOUT}
}

metrics {
  active = false
  active = ${?COMET_METRICS_ACTIVE}

  path = "/tmp/metrics/{domain}/{schema}"
  path = ${?COMET_METRICS_PATH}

  discrete-max-cardinality = 10
  discrete-max-cardinality = ${?COMET_METRICS_DISCRETE_MAX_CARDINALITY}

  infer = true
  infer = ${?COMET_METRICS_INFER}

  max-errors = 100
  index = "NONE"
  options = {
    "bq-dataset": "audit"
  }

}


audit {
  active = true
  active = ${?COMET_AUDIT_ACTIVE}

  #  path = "/tmp/metrics/{domain}/{schema}"
  path = "/tmp/audit"
  path = ${?COMET_AUDIT_PATH}

  audit-timeout = -1
  audit-timeout = ${?COMET_LOCK_AUDIT_TIMEOUT}

  max-errors = 100
  index = "NONE"
  options = {
    "bq-dataset": "audit",
    "jdbc-uri": "",
    "jdbc-user": "",
    "jdbc-password": ""
  }

}

area {
  pending = "pending"
  pending = ${?COMET_PENDING}
  unresolved = "unresolved"
  unresolved = ${?COMET_UNRESOLVED}
  archive = "archive"
  archive = ${?COMET_ARCHIVE}
  ingesting = "ingesting"
  ingesting = ${?COMET_INGESTING}
  accepted = "accepted"
  accepted = ${?COMET_ACCEPTED}
  rejected = "rejected"
  rejected = ${?COMET_REJECTED}
  business = "business"
  business = ${?COMET_BUSINESS}
}

privacy {
  options = {
    "none": "com.ebiznext.comet.utils.No",
    "hide": "com.ebiznext.comet.utils.Hide",
    "md5": "com.ebiznext.comet.utils.Md5",
    "sha1": "com.ebiznext.comet.utils.Sha1",
    "sha256": "com.ebiznext.comet.utils.Sha256",
    "sha512": "com.ebiznext.comet.utils.Sha512"
  }
}

encryption-types {
  "PAN" : 50,
  "iban": 50
}

elasticsearch {
  active = false
  options = {
    "es.nodes": "localhost"
    "es.port": "9200",

    #  net.http.auth.user = ""
    #  net.http.auth.pass = ""

    "es.net.ssl": "false",
    "es.net.ssl.cert.allow.self.signed": "false",

    "es.batch.size.entries": "1000",
    "es.batch.size.bytes": "1mb",
    "es.batch.write.retry.count": "3",
    "es.batch.write.retry.wait": "10s"
  }
}

spark {
  #  sql.hive.convertMetastoreParquet = false
  #  extraListeners = com.hortonworks.spark.atlas.SparkAtlasEventTracker
  #  sql.queryExecutionListeners = com.hortonworks.spark.atlas.SparkAtlasEventTracker
  #  yarn.principal = "invalid"
  #  yarn.keytab = "invalid"
  #  yarn.principal = ${?SPARK_YARN_PRINCIPAL}
  #  yarn.keytab = ${?SPARK_YARN_KEYTAB}
  debug.maxToStringFields=100
  master = "local[*]"
  #  sql.catalogImplementation="hive"
  #  sql.catalogImplementation="in-memory"
}
